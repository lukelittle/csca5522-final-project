{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Download and Preprocessing\n",
    "\n",
    "**Author:** Lucas Little  \n",
    "**Course:** CSCA 5522: Data Mining Project  \n",
    "**University:** University of Colorado - Boulder\n",
    "\n",
    "This notebook handles the initial data acquisition and preprocessing for the cryptocurrency sentiment analysis project.\n",
    "\n",
    "## Objectives\n",
    "1. Download and load Bitcoin tweets dataset from Kaggle\n",
    "2. Download and load Bitcoin historical price data\n",
    "3. Perform initial data exploration and quality assessment\n",
    "4. Clean and preprocess the datasets\n",
    "5. Save processed data for subsequent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created/verified: ../data\n",
      "Directory created/verified: ../data/raw\n",
      "Directory created/verified: ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# Create data directory structure\n",
    "data_dir = Path('../data') # Relative to the notebook directory\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [data_dir, raw_data_dir, processed_data_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "    print(f\"Directory created/verified: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /Users/luke/.cache/kagglehub/datasets/alaix14/bitcoin-tweets-20160101-to-20190329/versions/2/tweets.csv to ../data/raw/tweets.csv\n",
      "Copied /Users/luke/.cache/kagglehub/datasets/mczielinski/bitcoin-historical-data/versions/287/btcusd_1-min_data.csv to ../data/raw/btcusd_1-min_data.csv\n",
      "‚úÖ Datasets downloaded and copied to data/raw directory.\n"
     ]
    }
   ],
   "source": [
    "# Install kagglehub\n",
    "!pip install -q kagglehub\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "tweet_path = kagglehub.dataset_download(\"alaix14/bitcoin-tweets-20160101-to-20190329\")\n",
    "price_path = kagglehub.dataset_download(\"mczielinski/bitcoin-historical-data\")\n",
    "\n",
    "# Copy files to the data/raw directory\n",
    "def copy_files(src_dir, dest_dir):\n",
    "    for item in os.listdir(src_dir):\n",
    "        s = os.path.join(src_dir, item)\n",
    "        d = os.path.join(dest_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            copy_files(s, dest_dir)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "            print(f\"Copied {s} to {d}\")\n",
    "\n",
    "copy_files(tweet_path, raw_data_dir)\n",
    "copy_files(price_path, raw_data_dir)\n",
    "\n",
    "print(\"‚úÖ Datasets downloaded and copied to data/raw directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading real datasets...\n",
      "‚úÖ Loaded real datasets from Kaggle\n",
      "\n",
      "Dataset Summary:\n",
      "Tweet data shape: (16890422, 9)\n",
      "Price data shape: (7103245, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load real datasets from Kaggle\n",
    "print(\"üìä Loading real datasets...\")\n",
    "try:\n",
    "    tweet_chunks = pd.read_csv(raw_data_dir / 'tweets.csv', delimiter=';', on_bad_lines='skip', engine='python', chunksize=100000)\n",
    "    tweet_data = pd.concat([chunk for chunk in tweet_chunks])\n",
    "    price_data = pd.read_csv(raw_data_dir / 'btcusd_1-min_data.csv')\n",
    "    print(\"‚úÖ Loaded real datasets from Kaggle\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    print(\"Please make sure the Kaggle datasets are downloaded and unzipped in the 'data/raw' directory.\")\n",
    "    tweet_data, price_data = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "if not tweet_data.empty and not price_data.empty:\n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"Tweet data shape: {tweet_data.shape}\")\n",
    "    print(f\"Price data shape: {price_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n",
      "‚úÖ Preprocessing complete!\n",
      "Final tweet data shape: (16889041, 7)\n",
      "Final price data shape: (7103245, 6)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweets(df):\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    \n",
    "    df = df[['timestamp', 'text', 'user_name', 'reply_count', 'like_count', 'retweet_count', 'tweet_id']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_prices(df):\n",
    "    df = df[['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        'Open': 'open', \n",
    "        'High': 'high', \n",
    "        'Low': 'low', \n",
    "        'Close': 'close',\n",
    "        'Volume': 'volume'\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "if not tweet_data.empty and not price_data.empty:\n",
    "    print(\"Preprocessing datasets...\")\n",
    "    tweet_data = preprocess_tweets(tweet_data)\n",
    "    price_data = preprocess_prices(price_data)\n",
    "    print(\"‚úÖ Preprocessing complete!\")\n",
    "    \n",
    "    print(f\"Final tweet data shape: {tweet_data.shape}\")\n",
    "    print(f\"Final price data shape: {price_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving processed datasets...\n",
      "‚úÖ Saved processed tweets: ../data/processed/tweets_processed.csv\n",
      "‚úÖ Saved processed prices: ../data/processed/prices_processed.csv\n",
      "‚úÖ Saved metadata: ../data/processed/metadata.json\n",
      "\n",
      "üéâ Data preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "if not tweet_data.empty and not price_data.empty:\n",
    "    print(\"üíæ Saving processed datasets...\")\n",
    "    \n",
    "    # Save tweet data\n",
    "    tweet_output_path = processed_data_dir / 'tweets_processed.csv'\n",
    "    tweet_data.to_csv(tweet_output_path, index=False)\n",
    "    print(f\"‚úÖ Saved processed tweets: {tweet_output_path}\")\n",
    "    \n",
    "    # Save price data\n",
    "    price_output_path = processed_data_dir / 'prices_processed.csv'\n",
    "    price_data.to_csv(price_output_path, index=False)\n",
    "    print(f\"‚úÖ Saved processed prices: {price_output_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'tweet_count': len(tweet_data),\n",
    "        'price_count': len(price_data),\n",
    "        'date_range': {\n",
    "            'start': tweet_data['timestamp'].min().isoformat(),\n",
    "            'end': tweet_data['timestamp'].max().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = processed_data_dir / 'metadata.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved metadata: {metadata_path}\")\n",
    "    print(\"\\nüéâ Data preprocessing complete!\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
