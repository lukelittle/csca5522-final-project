{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Modeling and Backtesting\n",
    "\n",
    "**Author:** Lucas Little  \n",
    "**Course:** CSCA 5522: Data Mining Project  \n",
    "**University:** University of Colorado - Boulder\n",
    "\n",
    "This notebook implements machine learning models for cryptocurrency volatility prediction and backtesting, incorporating sentiment analysis features on the sampled data.\n",
    "\n",
    "## Objectives\n",
    "1. Load enhanced feature sets for each sample\n",
    "2. Implement multiple ML models for volatility prediction on each sample\n",
    "3. Compare sentiment-enhanced model with a technical-only baseline across all samples\n",
    "4. Perform time series cross-validation and evaluate performance\n",
    "5. Implement backtesting strategy and analyze aggregated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ML imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Sampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data')\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "sampled_dir = processed_data_dir / 'sampled'\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(f\"\\n--- Processing Sample {i} ---\")\n",
    "    enhanced_features_path = sampled_dir / f'enhanced_features_sample_{i}.csv'\n",
    "    \n",
    "    if not enhanced_features_path.exists():\n",
    "        print(f\"⚠️ Enhanced features for sample {i} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    df = pd.read_csv(enhanced_features_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Define feature sets\n",
    "    tech_features = ['returns', 'volatility', 'rsi', 'macd', 'volume_ratio']\n",
    "    sentiment_features = tech_features + ['sentiment_mean', 'sentiment_var', 'sentiment_count', 'sentiment_momentum', 'sentiment_mean_anomaly']\n",
    "    target_col = 'high_volatility_target'\n",
    "    \n",
    "    # Prepare data\n",
    "    X_tech = df[tech_features]\n",
    "    X_sentiment = df[sentiment_features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_tech_scaled = scaler.fit_transform(X_tech)\n",
    "    X_sentiment_scaled = scaler.fit_transform(X_sentiment)\n",
    "    \n",
    "    # Time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Evaluate models\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name} on sample {i}...\")\n",
    "        \n",
    "        # Technical features only\n",
    "        cv_scores_tech = []\n",
    "        for train_index, test_index in tscv.split(X_tech_scaled):\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            if len(np.unique(y_test)) < 2:\n",
    "                print(\"Skipping split with only one class.\")\n",
    "                continue\n",
    "            model.fit(X_tech_scaled[train_index], y_train)\n",
    "            y_pred_proba = model.predict_proba(X_tech_scaled[test_index])[:, 1]\n",
    "            cv_scores_tech.append(roc_auc_score(y_test, y_pred_proba))\n",
    "        \n",
    "        # With sentiment features\n",
    "        cv_scores_sentiment = []\n",
    "        for train_index, test_index in tscv.split(X_sentiment_scaled):\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            if len(np.unique(y_test)) < 2:\n",
    "                print(\"Skipping split with only one class.\")\n",
    "                continue\n",
    "            model.fit(X_sentiment_scaled[train_index], y_train)\n",
    "            y_pred_proba = model.predict_proba(X_sentiment_scaled[test_index])[:, 1]\n",
    "            cv_scores_sentiment.append(roc_auc_score(y_test, y_pred_proba))\n",
    "        \n",
    "        all_results.append({\n",
    "            'sample': i,\n",
    "            'model': name,\n",
    "            'tech_only_auc': np.mean(cv_scores_tech) if cv_scores_tech else np.nan,\n",
    "            'sentiment_auc': np.mean(cv_scores_sentiment) if cv_scores_sentiment else np.nan\n",
    "        })\n",
    "        print(f\"  Tech-only AUC: {np.mean(cv_scores_tech):.4f}\")\n",
    "        print(f\"  Sentiment AUC: {np.mean(cv_scores_sentiment):.4f}\")\n",
    "\n",
    "# Display aggregated results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\n=== AGGREGATED MODEL COMPARISON ===\")\n",
    "print(results_df.groupby('model').mean())\n",
    "\n",
    "# Save aggregated results\n",
    "output_path = processed_data_dir / 'aggregated_model_results.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Aggregated model results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
