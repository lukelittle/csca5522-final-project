{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Anomaly Detection and Feature Engineering\n",
    "\n",
    "**Author:** Lucas Little  \n",
    "**Course:** CSCA 5522: Data Mining Project  \n",
    "**University:** University of Colorado - Boulder\n",
    "\n",
    "This notebook implements STL decomposition and z-score analysis for sentiment anomaly detection and creates the final feature set for each sample.\n",
    "\n",
    "## Objectives\n",
    "1. Load aligned price and sentiment data for each sample\n",
    "2. Implement STL decomposition for sentiment signals for each sample\n",
    "3. Detect sentiment anomalies using z-scores for each sample\n",
    "4. Engineer final features for modeling for each sample\n",
    "5. Save the enhanced feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T18:34:01.007383Z",
     "iopub.status.busy": "2025-07-05T18:34:01.007292Z",
     "iopub.status.idle": "2025-07-05T18:34:01.697805Z",
     "shell.execute_reply": "2025-07-05T18:34:01.697521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Time series analysis imports\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Sampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T18:34:01.713185Z",
     "iopub.status.busy": "2025-07-05T18:34:01.713037Z",
     "iopub.status.idle": "2025-07-05T18:34:02.234687Z",
     "shell.execute_reply": "2025-07-05T18:34:02.234436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Sample 1 ---\n",
      "Detecting sentiment anomalies...\n",
      "Detected 28 sentiment anomalies (4.16%)\n",
      "Using 10 features for modeling.\n",
      "Shape of the final modeling dataset for sample 1: (672, 11)\n",
      "Enhanced dataset for sample 1 saved to: ../data/processed/sampled/features_sample_1.csv\n",
      "\n",
      "--- Processing Sample 2 ---\n",
      "Detecting sentiment anomalies...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 25 sentiment anomalies (3.71%)\n",
      "Using 10 features for modeling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final modeling dataset for sample 2: (404, 11)\n",
      "Enhanced dataset for sample 2 saved to: ../data/processed/sampled/features_sample_2.csv\n",
      "\n",
      "--- Processing Sample 3 ---\n",
      "Detecting sentiment anomalies...\n",
      "Detected 26 sentiment anomalies (3.86%)\n",
      "Using 10 features for modeling.\n",
      "Shape of the final modeling dataset for sample 3: (672, 11)\n",
      "Enhanced dataset for sample 3 saved to: ../data/processed/sampled/features_sample_3.csv\n",
      "\n",
      "--- Processing Sample 4 ---\n",
      "Detecting sentiment anomalies...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 32 sentiment anomalies (4.75%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 10 features for modeling.\n",
      "Shape of the final modeling dataset for sample 4: (672, 11)\n",
      "Enhanced dataset for sample 4 saved to: ../data/processed/sampled/features_sample_4.csv\n",
      "\n",
      "--- Processing Sample 5 ---\n",
      "Detecting sentiment anomalies...\n",
      "Detected 30 sentiment anomalies (4.46%)\n",
      "Using 10 features for modeling.\n",
      "Shape of the final modeling dataset for sample 5: (672, 11)\n",
      "Enhanced dataset for sample 5 saved to: ../data/processed/sampled/features_sample_5.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('../data')\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "sampled_dir = processed_data_dir / 'sampled'\n",
    "\n",
    "def detect_sentiment_anomalies(df, sentiment_col='sentiment_mean', z_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Detect anomalies in a sentiment time series using STL decomposition and Z-scores.\n",
    "    \"\"\"\n",
    "    if sentiment_col not in df.columns or df[sentiment_col].isnull().all():\n",
    "        print(f\"Sentiment column '{sentiment_col}' not found or is all NaN.\")\n",
    "        return df, None\n",
    "    \n",
    "    # STL decomposition requires a period. For 15-min data, a daily seasonality is 24*4 = 96 periods.\n",
    "    stl = STL(df[sentiment_col].fillna(0), period=96, robust=True)\n",
    "    result = stl.fit()\n",
    "    \n",
    "    # Get the residual component\n",
    "    residuals = result.resid\n",
    "    \n",
    "    # Calculate Z-scores of the residuals\n",
    "    z_scores = np.abs(stats.zscore(residuals))\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_result = df.copy()\n",
    "    df_result[f'{sentiment_col}_stl_residual'] = residuals\n",
    "    df_result[f'{sentiment_col}_z_score'] = z_scores\n",
    "    df_result[f'{sentiment_col}_anomaly'] = (z_scores > z_threshold).astype(int)\n",
    "    \n",
    "    return df_result, result\n",
    "\n",
    "for i in range(1, 6):\n",
    "    print(f\"\\n--- Processing Sample {i} ---\")\n",
    "    aligned_features_path = sampled_dir / f'aligned_features_sample_{i}.csv'\n",
    "    \n",
    "    if not aligned_features_path.exists():\n",
    "        print(f\"⚠️ Aligned features for sample {i} not found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    df = pd.read_csv(aligned_features_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    print(\"Detecting sentiment anomalies...\")\n",
    "    df, stl_result = detect_sentiment_anomalies(df, 'sentiment_mean', z_threshold=2.5)\n",
    "    \n",
    "    if stl_result:\n",
    "        anomaly_count = df['sentiment_mean_anomaly'].sum()\n",
    "        anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "        print(f\"Detected {anomaly_count} sentiment anomalies ({anomaly_pct:.2f}%)\")\n",
    "    \n",
    "    # Create target variable: high volatility in the next 2 hours\n",
    "    df['future_volatility'] = df['volatility'].shift(-8) # 8 * 15 min = 120 min = 2 hours\n",
    "    df['high_volatility_target'] = (df['future_volatility'] > df['volatility'].quantile(0.75)).astype(int)\n",
    "    \n",
    "    # Feature columns\n",
    "    feature_cols = [\n",
    "        'returns',\n",
    "        'volatility',\n",
    "        'rsi',\n",
    "        'macd',\n",
    "        'volume_ratio',\n",
    "        'sentiment_mean',\n",
    "        'sentiment_var',\n",
    "        'sentiment_count',\n",
    "        'sentiment_momentum',\n",
    "        'sentiment_mean_anomaly'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all feature columns exist\n",
    "    final_features = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"Using {len(final_features)} features for modeling.\")\n",
    "    \n",
    "    # Drop rows with NaN in target or features\n",
    "    df_model = df[final_features + ['high_volatility_target']].dropna()\n",
    "    \n",
    "    print(f\"Shape of the final modeling dataset for sample {i}: {df_model.shape}\")\n",
    "    \n",
    "    # Save the enhanced dataset\n",
    "    output_path = sampled_dir / f'features_sample_{i}.csv'\n",
    "    df_model.to_csv(output_path, index=True)\n",
    "    print(f\"Enhanced dataset for sample {i} saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
