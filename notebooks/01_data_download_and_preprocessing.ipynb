{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Download and Preprocessing\n",
    "\n",
    "**Author:** Lucas Little  \n",
    "**Course:** CSCA 5522: Data Mining Project  \n",
    "**University:** University of Colorado - Boulder\n",
    "\n",
    "This notebook handles the initial data acquisition and preprocessing for the cryptocurrency sentiment analysis project.\n",
    "\n",
    "## Objectives\n",
    "1. Download and load Bitcoin tweets dataset from Kaggle\n",
    "2. Download and load Bitcoin historical price data\n",
    "3. Perform initial data exploration and quality assessment\n",
    "4. Clean and preprocess the datasets\n",
    "5. Save processed data for subsequent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Current working directory: /Users/luke/Desktop/data mining project/notebook\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created/verified: data\n",
      "Directory created/verified: data/raw\n",
      "Directory created/verified: data/processed\n",
      "\n",
      "Data directory structure:\n",
      "data/\n",
      "â”œâ”€â”€ raw/          # Original datasets from Kaggle\n",
      "â””â”€â”€ processed/    # Cleaned and processed datasets\n"
     ]
    }
   ],
   "source": [
    "# Create data directory structure\n",
    "data_dir = Path('data')\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [data_dir, raw_data_dir, processed_data_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "    print(f\"Directory created/verified: {directory}\")\n",
    "\n",
    "print(\"\\nData directory structure:\")\n",
    "print(\"data/\")\n",
    "print(\"â”œâ”€â”€ raw/          # Original datasets from Kaggle\")\n",
    "print(\"â””â”€â”€ processed/    # Cleaned and processed datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def create_sample_bitcoin_tweets(n_tweets=50000, start_date='2018-01-01', end_date='2019-03-29'):\n",
    "    \"\"\"\n",
    "    Create sample Bitcoin tweets dataset for demonstration.\n",
    "    In production, replace with actual Kaggle dataset loading.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample tweet templates with varying sentiment\n",
    "    positive_tweets = [\n",
    "        \"Bitcoin is going to the moon! ðŸš€ #BTC #crypto #bullish\",\n",
    "        \"HODL strong! Bitcoin will recover soon #diamondhands\",\n",
    "        \"Bitcoin breaking resistance levels! Bullish! #BTC\",\n",
    "        \"Buying the Bitcoin dip, great opportunity #crypto\",\n",
    "        \"Institutional adoption driving Bitcoin up #bitcoin\",\n",
    "        \"Bitcoin whale accumulation detected! Bullish signal\",\n",
    "        \"Technical analysis shows Bitcoin reversal incoming\",\n",
    "        \"Bitcoin hashrate hitting new highs! Network strong\",\n",
    "        \"Major companies adding Bitcoin to balance sheet\",\n",
    "        \"Bitcoin Lightning Network adoption growing fast\"\n",
    "    ]\n",
    "    \n",
    "    negative_tweets = [\n",
    "        \"Bearish on Bitcoin today, expecting a dip #crypto\",\n",
    "        \"Selling my Bitcoin, market looks scary #bearish\",\n",
    "        \"Crypto winter is here, Bitcoin falling hard\",\n",
    "        \"Bitcoin correlation with stocks concerning #risk\",\n",
    "        \"Regulatory news impacting Bitcoin price negatively\",\n",
    "        \"Bitcoin energy consumption criticism mounting\",\n",
    "        \"Market manipulation in Bitcoin obvious today\",\n",
    "        \"Bitcoin transaction fees too high for adoption\",\n",
    "        \"Tether concerns affecting Bitcoin confidence\",\n",
    "        \"Bitcoin technical indicators showing weakness\"\n",
    "    ]\n",
    "    \n",
    "    neutral_tweets = [\n",
    "        \"Bitcoin volatility is insane today #crypto\",\n",
    "        \"Bitcoin mining difficulty adjustment coming\",\n",
    "        \"Watching Bitcoin price action closely today\",\n",
    "        \"Bitcoin options expiry this Friday #derivatives\",\n",
    "        \"Bitcoin dominance at interesting levels\",\n",
    "        \"Bitcoin halving cycle analysis #crypto\",\n",
    "        \"Bitcoin on-chain metrics looking neutral\",\n",
    "        \"Bitcoin futures market showing mixed signals\",\n",
    "        \"Bitcoin ETF news still pending approval\",\n",
    "        \"Bitcoin developer activity remains strong\"\n",
    "    ]\n",
    "    \n",
    "    all_tweets = positive_tweets + negative_tweets + neutral_tweets\n",
    "    \n",
    "    # Generate timestamps\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    timestamps = pd.date_range(start, end, periods=n_tweets)\n",
    "    \n",
    "    # Generate tweet data\n",
    "    tweets = np.random.choice(all_tweets, n_tweets)\n",
    "    \n",
    "    # Add realistic metadata\n",
    "    tweet_data = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'text': tweets,\n",
    "        'user_followers': np.random.exponential(1000, n_tweets).astype(int),\n",
    "        'retweet_count': np.random.poisson(5, n_tweets),\n",
    "        'like_count': np.random.poisson(10, n_tweets),\n",
    "        'user_verified': np.random.choice([True, False], n_tweets, p=[0.1, 0.9]),\n",
    "        'tweet_id': [f\"tweet_{i}\" for i in range(n_tweets)]\n",
    "    })\n",
    "    \n",
    "    return tweet_data\n",
    "\n",
    "def create_sample_bitcoin_prices(start_date='2018-01-01', end_date='2019-03-29', freq='1min'):\n",
    "    \"\"\"\n",
    "    Create sample Bitcoin price data for demonstration.\n",
    "    In production, replace with actual Kaggle dataset loading.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate timestamps\n",
    "    timestamps = pd.date_range(start_date, end_date, freq=freq)\n",
    "    n_periods = len(timestamps)\n",
    "    \n",
    "    # Simulate realistic Bitcoin price movements\n",
    "    initial_price = 10000  # Starting price around $10,000\n",
    "    \n",
    "    # Generate returns with realistic volatility\n",
    "    daily_vol = 0.04  # 4% daily volatility\n",
    "    minute_vol = daily_vol / np.sqrt(24 * 60)  # Scale to minute volatility\n",
    "    \n",
    "    returns = np.random.normal(0, minute_vol, n_periods)\n",
    "    \n",
    "    # Add some trend and mean reversion\n",
    "    trend = np.linspace(0, 0.5, n_periods)  # Slight upward trend\n",
    "    returns += trend / n_periods\n",
    "    \n",
    "    # Calculate prices\n",
    "    log_prices = np.log(initial_price) + np.cumsum(returns)\n",
    "    prices = np.exp(log_prices)\n",
    "    \n",
    "    # Generate OHLCV data\n",
    "    price_data = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'open': prices,\n",
    "        'high': prices * (1 + np.abs(np.random.normal(0, 0.002, n_periods))),\n",
    "        'low': prices * (1 - np.abs(np.random.normal(0, 0.002, n_periods))),\n",
    "        'close': prices,\n",
    "        'volume': np.random.exponential(1000000, n_periods)\n",
    "    })\n",
    "    \n",
    "    # Ensure high >= close >= low and high >= open >= low\n",
    "    price_data['high'] = np.maximum(price_data['high'], \n",
    "                                   np.maximum(price_data['open'], price_data['close']))\n",
    "    price_data['low'] = np.minimum(price_data['low'], \n",
    "                                  np.minimum(price_data['open'], price_data['close']))\n",
    "    \n",
    "    return price_data\n",
    "\n",
    "print(\"Data generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Creating sample datasets for demonstration...\n",
      "   (Replace with real Kaggle data loading in production)\n",
      "âœ… Sample datasets created successfully\n",
      "\n",
      "Dataset Summary:\n",
      "Tweet data shape: (50000, 7)\n",
      "Price data shape: (650881, 6)\n",
      "Tweet date range: 2018-01-01 00:00:00 to 2019-03-29 00:00:00\n",
      "Price date range: 2018-01-01 00:00:00 to 2019-03-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Try to load real datasets, fall back to sample data\n",
    "try:\n",
    "    # Attempt to load real Kaggle datasets\n",
    "    # Uncomment and modify these paths when you have the actual datasets\n",
    "    \n",
    "    # tweet_data = pd.read_csv(raw_data_dir / 'bitcoin_tweets.csv')\n",
    "    # price_data = pd.read_csv(raw_data_dir / 'bitcoin_prices.csv')\n",
    "    # print(\"âœ… Loaded real datasets from Kaggle\")\n",
    "    \n",
    "    # For now, create sample data\n",
    "    raise FileNotFoundError(\"Using sample data for demonstration\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ðŸ“Š Creating sample datasets for demonstration...\")\n",
    "    print(\"   (Replace with real Kaggle data loading in production)\")\n",
    "    \n",
    "    # Create sample datasets\n",
    "    tweet_data = create_sample_bitcoin_tweets(n_tweets=50000)\n",
    "    price_data = create_sample_bitcoin_prices()\n",
    "    \n",
    "    print(\"âœ… Sample datasets created successfully\")\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Tweet data shape: {tweet_data.shape}\")\n",
    "print(f\"Price data shape: {price_data.shape}\")\n",
    "print(f\"Tweet date range: {tweet_data['timestamp'].min()} to {tweet_data['timestamp'].max()}\")\n",
    "print(f\"Price date range: {price_data['timestamp'].min()} to {price_data['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving processed datasets...\n",
      "âœ… Saved processed tweets: data/processed/tweets_processed.csv\n",
      "âœ… Saved processed prices: data/processed/prices_processed.csv\n",
      "âœ… Saved metadata: data/processed/metadata.json\n",
      "\n",
      "ðŸŽ‰ Data preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "print(\"ðŸ’¾ Saving processed datasets...\")\n",
    "\n",
    "# Save tweet data\n",
    "tweet_output_path = processed_data_dir / 'tweets_processed.csv'\n",
    "tweet_data.to_csv(tweet_output_path, index=False)\n",
    "print(f\"âœ… Saved processed tweets: {tweet_output_path}\")\n",
    "\n",
    "# Save price data\n",
    "price_output_path = processed_data_dir / 'prices_processed.csv'\n",
    "price_data.to_csv(price_output_path, index=False)\n",
    "print(f\"âœ… Saved processed prices: {price_output_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'tweet_count': len(tweet_data),\n",
    "    'price_count': len(price_data),\n",
    "    'date_range': {\n",
    "        'start': tweet_data['timestamp'].min().isoformat(),\n",
    "        'end': tweet_data['timestamp'].max().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = processed_data_dir / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved metadata: {metadata_path}\")\n",
    "print(\"\\nðŸŽ‰ Data preprocessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
