{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Anomaly Detection and Feature Engineering\n",
    "\n",
    "**Author:** Lucas Little  \n",
    "**Course:** CSCA 5522: Data Mining Project  \n",
    "**University:** University of Colorado - Boulder\n",
    "\n",
    "This notebook implements advanced anomaly detection and feature engineering for cryptocurrency volatility prediction.\n",
    "\n",
    "## Objectives\n",
    "1. Implement anomaly detection algorithms\n",
    "2. Create advanced engineered features\n",
    "3. Detect market regime changes\n",
    "4. Generate interaction features\n",
    "5. Prepare final feature set for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "# Statistical imports\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded aligned features: (650881, 101)\n",
      "Date range: 2018-01-01 00:00:00 to 2019-03-29 00:00:00\n",
      "Features available: 101\n",
      "Dataset shape: (650881, 101)\n"
     ]
    }
   ],
   "source": [
    "# Load aligned features from previous notebook\n",
    "data_dir = Path('data')\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "\n",
    "try:\n",
    "    # Try to load aligned features\n",
    "    df = pd.read_csv(processed_data_dir / 'aligned_features.csv')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    print(f\"Loaded aligned features: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback: load and create basic features\n",
    "    print(\"Aligned features not found, creating from price data...\")\n",
    "    \n",
    "    # Load price data\n",
    "    prices_df = pd.read_csv(processed_data_dir / 'prices_processed.csv')\n",
    "    prices_df['timestamp'] = pd.to_datetime(prices_df['timestamp'])\n",
    "    \n",
    "    # Create basic features\n",
    "    df = prices_df.copy()\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "    \n",
    "    # Add dummy sentiment features\n",
    "    np.random.seed(42)\n",
    "    df['primary_sentiment_mean'] = np.random.normal(0, 0.1, len(df))\n",
    "    df['primary_sentiment_std'] = np.random.uniform(0.05, 0.2, len(df))\n",
    "    df['tweet_count'] = np.random.poisson(10, len(df))\n",
    "    \n",
    "    print(f\"Created basic features: {df.shape}\")\n",
    "\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Features available: {len(df.columns)}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting anomalies...\n",
      "Isolation Forest anomalies: 32543 (5.00%)\n",
      "Returns Z-score anomalies: 1730 (0.27%)\n",
      "Volume Z-score anomalies: 7726 (1.19%)\n",
      "\n",
      "Dataset shape after anomaly detection: (650881, 107)\n"
     ]
    }
   ],
   "source": [
    "def detect_price_anomalies(df, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Detect price and volume anomalies using Isolation Forest.\n",
    "    \"\"\"\n",
    "    # Select features for anomaly detection\n",
    "    anomaly_features = ['returns', 'volatility', 'volume_ratio']\n",
    "    available_features = [f for f in anomaly_features if f in df.columns]\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(\"No suitable features for anomaly detection\")\n",
    "        return df\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[available_features].dropna()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"No valid data for anomaly detection\")\n",
    "        return df\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    \n",
    "    anomaly_labels = iso_forest.fit_predict(X_scaled)\n",
    "    anomaly_scores = iso_forest.score_samples(X_scaled)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_result = df.copy()\n",
    "    df_result['anomaly_score'] = np.nan\n",
    "    df_result['is_anomaly'] = 0\n",
    "    \n",
    "    # Map results back to original dataframe\n",
    "    df_result.loc[X.index, 'anomaly_score'] = anomaly_scores\n",
    "    df_result.loc[X.index, 'is_anomaly'] = (anomaly_labels == -1).astype(int)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def detect_statistical_anomalies(df, z_threshold=3):\n",
    "    \"\"\"\n",
    "    Detect statistical anomalies using Z-score method.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Z-score anomalies for returns\n",
    "    if 'returns' in df.columns:\n",
    "        returns_clean = df['returns'].dropna()\n",
    "        if len(returns_clean) > 0:\n",
    "            z_scores = np.abs(stats.zscore(returns_clean))\n",
    "            df_result['returns_z_score'] = np.nan\n",
    "            df_result.loc[returns_clean.index, 'returns_z_score'] = z_scores\n",
    "            df_result['returns_anomaly'] = (df_result['returns_z_score'] > z_threshold).astype(int)\n",
    "    \n",
    "    # Volume anomalies\n",
    "    if 'volume' in df.columns:\n",
    "        volume_clean = df['volume'].dropna()\n",
    "        if len(volume_clean) > 0:\n",
    "            # Use log transformation for volume\n",
    "            log_volume = np.log1p(volume_clean)\n",
    "            z_scores_vol = np.abs(stats.zscore(log_volume))\n",
    "            df_result['volume_z_score'] = np.nan\n",
    "            df_result.loc[volume_clean.index, 'volume_z_score'] = z_scores_vol\n",
    "            df_result['volume_anomaly'] = (df_result['volume_z_score'] > z_threshold).astype(int)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"Detecting anomalies...\")\n",
    "\n",
    "# Apply anomaly detection\n",
    "df = detect_price_anomalies(df, contamination=0.05)\n",
    "df = detect_statistical_anomalies(df, z_threshold=3)\n",
    "\n",
    "# Summary of anomalies\n",
    "if 'is_anomaly' in df.columns:\n",
    "    anomaly_count = df['is_anomaly'].sum()\n",
    "    anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "    print(f\"Isolation Forest anomalies: {anomaly_count} ({anomaly_pct:.2f}%)\")\n",
    "\n",
    "if 'returns_anomaly' in df.columns:\n",
    "    returns_anomaly_count = df['returns_anomaly'].sum()\n",
    "    returns_anomaly_pct = (returns_anomaly_count / len(df)) * 100\n",
    "    print(f\"Returns Z-score anomalies: {returns_anomaly_count} ({returns_anomaly_pct:.2f}%)\")\n",
    "\n",
    "if 'volume_anomaly' in df.columns:\n",
    "    volume_anomaly_count = df['volume_anomaly'].sum()\n",
    "    volume_anomaly_pct = (volume_anomaly_count / len(df)) * 100\n",
    "    print(f\"Volume Z-score anomalies: {volume_anomaly_count} ({volume_anomaly_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDataset shape after anomaly detection: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Market Regime Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting market regimes...\n",
      "\n",
      "Market Regime Distribution:\n",
      "  sideways: 650881 (100.0%)\n",
      "\n",
      "Dataset shape after regime detection: (650881, 114)\n"
     ]
    }
   ],
   "source": [
    "def detect_market_regimes(df, window=50):\n",
    "    \"\"\"\n",
    "    Detect market regimes (bull, bear, sideways) based on price trends.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if 'close' not in df.columns:\n",
    "        print(\"Close price not available for regime detection\")\n",
    "        return df_result\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df_result['price_sma'] = df_result['close'].rolling(window=window).mean()\n",
    "    df_result['price_trend'] = df_result['close'] / df_result['price_sma'] - 1\n",
    "    \n",
    "    # Calculate rolling volatility\n",
    "    if 'returns' in df.columns:\n",
    "        df_result['rolling_volatility'] = df_result['returns'].rolling(window=window).std()\n",
    "    \n",
    "    # Define regime thresholds\n",
    "    bull_threshold = 0.05  # 5% above SMA\n",
    "    bear_threshold = -0.05  # 5% below SMA\n",
    "    \n",
    "    # Classify regimes\n",
    "    conditions = [\n",
    "        df_result['price_trend'] > bull_threshold,\n",
    "        df_result['price_trend'] < bear_threshold\n",
    "    ]\n",
    "    choices = ['bull', 'bear']\n",
    "    \n",
    "    df_result['market_regime'] = np.select(conditions, choices, default='sideways')\n",
    "    \n",
    "    # Create regime dummy variables\n",
    "    df_result['regime_bull'] = (df_result['market_regime'] == 'bull').astype(int)\n",
    "    df_result['regime_bear'] = (df_result['market_regime'] == 'bear').astype(int)\n",
    "    df_result['regime_sideways'] = (df_result['market_regime'] == 'sideways').astype(int)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"Detecting market regimes...\")\n",
    "\n",
    "# Apply regime detection\n",
    "df = detect_market_regimes(df, window=50)\n",
    "\n",
    "# Summary of regimes\n",
    "if 'market_regime' in df.columns:\n",
    "    regime_counts = df['market_regime'].value_counts()\n",
    "    print(\"\\nMarket Regime Distribution:\")\n",
    "    for regime, count in regime_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {regime}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDataset shape after regime detection: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features...\n",
      "After interaction features: (650881, 119)\n",
      "After time features: (650881, 128)\n",
      "After momentum features: (650881, 136)\n",
      "\n",
      "Total features created: 136\n"
     ]
    }
   ],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features between different data types.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Price-Volume interactions\n",
    "    if all(col in df.columns for col in ['returns', 'volume_ratio']):\n",
    "        df_result['returns_volume_interaction'] = df_result['returns'] * df_result['volume_ratio']\n",
    "        df_result['abs_returns_volume'] = abs(df_result['returns']) * df_result['volume_ratio']\n",
    "    \n",
    "    # Price-Sentiment interactions\n",
    "    if all(col in df.columns for col in ['returns', 'primary_sentiment_mean']):\n",
    "        df_result['returns_sentiment_interaction'] = df_result['returns'] * df_result['primary_sentiment_mean']\n",
    "        df_result['sentiment_momentum'] = df_result['primary_sentiment_mean'] * df_result['returns'].rolling(5).mean()\n",
    "    \n",
    "    # Volume-Sentiment interactions\n",
    "    if all(col in df.columns for col in ['volume_ratio', 'tweet_count']):\n",
    "        df_result['volume_attention_interaction'] = df_result['volume_ratio'] * np.log1p(df_result['tweet_count'])\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def create_time_features(df):\n",
    "    \"\"\"\n",
    "    Create time-based features.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if 'timestamp' in df.columns:\n",
    "        # Extract time components\n",
    "        df_result['hour'] = df_result['timestamp'].dt.hour\n",
    "        df_result['day_of_week'] = df_result['timestamp'].dt.dayofweek\n",
    "        df_result['month'] = df_result['timestamp'].dt.month\n",
    "        \n",
    "        # Cyclical encoding\n",
    "        df_result['hour_sin'] = np.sin(2 * np.pi * df_result['hour'] / 24)\n",
    "        df_result['hour_cos'] = np.cos(2 * np.pi * df_result['hour'] / 24)\n",
    "        df_result['day_sin'] = np.sin(2 * np.pi * df_result['day_of_week'] / 7)\n",
    "        df_result['day_cos'] = np.cos(2 * np.pi * df_result['day_of_week'] / 7)\n",
    "        \n",
    "        # Market session indicators\n",
    "        df_result['is_weekend'] = (df_result['day_of_week'] >= 5).astype(int)\n",
    "        df_result['is_business_hours'] = ((df_result['hour'] >= 9) & (df_result['hour'] <= 17)).astype(int)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def create_momentum_features(df):\n",
    "    \"\"\"\n",
    "    Create momentum and trend features.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    if 'close' in df.columns:\n",
    "        # Price momentum\n",
    "        for period in [5, 10, 20]:\n",
    "            df_result[f'momentum_{period}'] = df_result['close'] / df_result['close'].shift(period) - 1\n",
    "    \n",
    "    if 'returns' in df.columns:\n",
    "        # Return momentum\n",
    "        for period in [3, 5, 10]:\n",
    "            df_result[f'return_momentum_{period}'] = df_result['returns'].rolling(period).mean()\n",
    "        \n",
    "        # Consecutive returns\n",
    "        df_result['positive_return_streak'] = (df_result['returns'] > 0).astype(int)\n",
    "        df_result['negative_return_streak'] = (df_result['returns'] < 0).astype(int)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"Creating advanced features...\")\n",
    "\n",
    "# Apply feature engineering\n",
    "df = create_interaction_features(df)\n",
    "print(f\"After interaction features: {df.shape}\")\n",
    "\n",
    "df = create_time_features(df)\n",
    "print(f\"After time features: {df.shape}\")\n",
    "\n",
    "df = create_momentum_features(df)\n",
    "print(f\"After momentum features: {df.shape}\")\n",
    "\n",
    "print(f\"\\nTotal features created: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Features by Correlation with returns:\n",
      "==================================================\n",
      " 1. returns                        1.0000\n",
      " 2. log_returns                    1.0000\n",
      " 3. negative_return_streak         0.7981\n",
      " 4. positive_return_streak         0.7981\n",
      " 5. returns_volume_interaction     0.7251\n",
      " 6. return_momentum_3              0.5771\n",
      " 7. bb_position                    0.4494\n",
      " 8. returns_rolling_mean_5         0.4477\n",
      " 9. return_momentum_5              0.4477\n",
      "10. momentum_5                     0.4477\n",
      "11. williams_r                     0.4413\n",
      "12. stoch_k                        0.4413\n",
      "13. cci                            0.4332\n",
      "14. rsi_14                         0.3591\n",
      "15. returns_rolling_mean_10        0.3164\n",
      "16. return_momentum_10             0.3164\n",
      "17. momentum_10                    0.3164\n",
      "18. returns_rolling_min_5          0.2994\n",
      "19. returns_rolling_max_5          0.2983\n",
      "20. rsi_30                         0.2519\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY SUMMARY\n",
      "============================================================\n",
      "Total samples: 650,881\n",
      "Total features: 136\n",
      "Date range: 2018-01-01 00:00:00 to 2019-03-29 00:00:00\n",
      "\n",
      "Features with missing values:\n",
      "  future_volatility_120m: 0.0%\n",
      "  future_returns_120m: 0.0%\n",
      "  sma_100: 0.0%\n",
      "  future_returns_60m: 0.0%\n",
      "  future_volatility_60m: 0.0%\n",
      "  rolling_volatility: 0.0%\n",
      "  realized_vol_50: 0.0%\n",
      "  volatility_50: 0.0%\n",
      "  price_trend: 0.0%\n",
      "  price_sma: 0.0%\n",
      "\n",
      "✅ Feature engineering complete!\n"
     ]
    }
   ],
   "source": [
    "def analyze_feature_importance(df, target_col='volatility'):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using correlation.\n",
    "    \"\"\"\n",
    "    # Identify feature columns\n",
    "    exclude_cols = ['timestamp', 'market_regime']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols and not col.startswith('future_')]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "    \n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Target column '{target_col}' not found. Using 'returns' as fallback.\")\n",
    "        target_col = 'returns' if 'returns' in df.columns else X.columns[0]\n",
    "    \n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Remove rows with missing target\n",
    "    valid_idx = ~(X.isnull().all(axis=1) | y.isnull())\n",
    "    X_clean = X.loc[valid_idx]\n",
    "    y_clean = y.loc[valid_idx]\n",
    "    \n",
    "    if len(X_clean) == 0:\n",
    "        print(\"No valid data for feature analysis\")\n",
    "        return\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    X_clean = X_clean.fillna(X_clean.median())\n",
    "    \n",
    "    # Calculate correlations\n",
    "    correlations = X_clean.corrwith(y_clean).abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Display top features\n",
    "    print(f\"\\nTop 20 Features by Correlation with {target_col}:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (feature, corr) in enumerate(correlations.head(20).items(), 1):\n",
    "        print(f\"{i:2d}. {feature:<30} {corr:.4f}\")\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Analyze feature importance\n",
    "if 'volatility' in df.columns:\n",
    "    correlations = analyze_feature_importance(df, 'volatility')\n",
    "else:\n",
    "    correlations = analyze_feature_importance(df, 'returns')\n",
    "\n",
    "# Data quality summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "\n",
    "# Missing values summary\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_features = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"\\nFeatures with missing values:\")\n",
    "    for feature, pct in missing_features.head(10).items():\n",
    "        print(f\"  {feature}: {pct:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected!\")\n",
    "\n",
    "print(\"\\n✅ Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset saved to: data/processed/enhanced_features.csv\n",
      "Feature categories saved to: data/processed/feature_categories.csv\n",
      "\n",
      "=== FEATURE ENGINEERING SUMMARY ===\n",
      "price_features: 13 features\n",
      "volume_features: 21 features\n",
      "returns_features: 30 features\n",
      "volatility_features: 40 features\n",
      "sentiment_features: 5 features\n",
      "anomaly_features: 6 features\n",
      "regime_features: 4 features\n",
      "time_features: 9 features\n",
      "momentum_features: 9 features\n",
      "interaction_features: 3 features\n",
      "\n",
      "Total engineered features: 140\n",
      "\n",
      "🎯 Ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "# Save the enhanced dataset\n",
    "output_path = processed_data_dir / 'enhanced_features.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Enhanced dataset saved to: {output_path}\")\n",
    "\n",
    "# Create feature categories for documentation\n",
    "feature_categories = {\n",
    "    'price_features': [col for col in df.columns if any(x in col.lower() for x in ['open', 'high', 'low', 'close', 'price'])],\n",
    "    'volume_features': [col for col in df.columns if 'volume' in col.lower()],\n",
    "    'returns_features': [col for col in df.columns if 'return' in col.lower()],\n",
    "    'volatility_features': [col for col in df.columns if 'volatility' in col.lower() or 'vol' in col.lower()],\n",
    "    'sentiment_features': [col for col in df.columns if 'sentiment' in col.lower() or 'tweet' in col.lower()],\n",
    "    'anomaly_features': [col for col in df.columns if 'anomaly' in col.lower() or 'z_score' in col.lower()],\n",
    "    'regime_features': [col for col in df.columns if 'regime' in col.lower()],\n",
    "    'time_features': [col for col in df.columns if any(x in col.lower() for x in ['hour', 'day', 'month', 'weekend', 'business'])],\n",
    "    'momentum_features': [col for col in df.columns if 'momentum' in col.lower() or 'streak' in col.lower()],\n",
    "    'interaction_features': [col for col in df.columns if 'interaction' in col.lower()]\n",
    "}\n",
    "\n",
    "# Save feature categories\n",
    "categories_summary = {}\n",
    "for category, features in feature_categories.items():\n",
    "    categories_summary[category] = len(features)\n",
    "    \n",
    "categories_df = pd.DataFrame([categories_summary])\n",
    "categories_path = processed_data_dir / 'feature_categories.csv'\n",
    "categories_df.to_csv(categories_path, index=False)\n",
    "print(f\"Feature categories saved to: {categories_path}\")\n",
    "\n",
    "print(\"\\n=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "for category, count in categories_summary.items():\n",
    "    print(f\"{category}: {count} features\")\n",
    "\n",
    "print(f\"\\nTotal engineered features: {sum(categories_summary.values())}\")\n",
    "print(\"\\n🎯 Ready for modeling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
